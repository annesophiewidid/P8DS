{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c424dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /home/ec2-user/anaconda3/lib/python3.8/site-packages (1.4.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a2fa4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/ec2-user/anaconda3/lib/python3.8/site-packages (3.2.0))\r\n",
      "Requirement already satisfied: py4j==0.10.9.2 in /home/ec2-user/anaconda3/lib/python3.8/site-packages (from pyspark) (0.10.9.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e2c481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3dc1f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'findspark' from '/home/ec2-user/anaconda3/lib/python3.8/site-packages/findspark.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ea5c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: conda: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!conda info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a6fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark modules\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, split\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT, DenseVector\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import StringIndexer, StandardScaler\n",
    "from pyspark.ml.feature import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8383d0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ba0e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"/home/ec2-user/spark-3.2.0-bin-hadoop3.2")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcd3e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-1.amzn2.0.2x86_64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/ec2-user/spark-3.2.0-bin-hadoop3.2""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a22d436",
   "metadata": {},
   "source": [
    "# 1.set up spark session et contexte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd61d1",
   "metadata": {},
   "source": [
    "spark = (SparkSession\n",
    "         .builder.master('local[*]')\n",
    "         .appName('p8cloud')\n",
    "         .config('spark.hadoop.fs.s3a.access.key', 'xxx6')\n",
    "         .config('spark.hadoop.fs.s3a.secret.key', 'xxx') \n",
    "         .config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') \n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0b90142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/quillaur/jupyter_notebooks/oc/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/17 16:06:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local[*]')\n",
    "         .appName('Cloud Modelâ€¦')\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f32b208",
   "metadata": {},
   "source": [
    "sc = spark.sparkContext\n",
    "sc.setSystemProperty('com.amazonaws.services.s3.enableV4', 'true')\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.eu-west-3.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9448de",
   "metadata": {},
   "source": [
    "# 2.importer les images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e81c1c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_url = \"s3a://asgp8bucket/dataset/**\"\n",
    "image_df = spark.read.format(\"binaryfile\").load(s3_url)\n",
    "image_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d52754ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|                path|             content| label|\n",
      "+--------------------+--------------------+------+\n",
      "|file:/asgp8bucket...|[FF D8 FF E0 00 1...| apple|\n",
      "|file:/asgp8bucket...|[FF D8 FF E0 00 1...| apple|\n",
      "|file:/asgp8bucket...|[FF D8 FF E0 00 1...| apple|\n",
      "|file:/asgp8bucket...|[FF D8 FF E0 00 1...|banana|\n",
      "|file:/asgp8bucket...|[FF D8 FF E0 00 1...|banana|\n",
      "|file:/asgp8bucket...|[FF D8 FF E0 00 1...|banana|\n",
      "+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_df = image_df.withColumn('label', split(col('path'), '/').getItem(6))\n",
    "\n",
    "image_df = image_df.select('path', 'content', 'label')\n",
    "image_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bb82c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               image|\n",
      "+--------------------+\n",
      "|{file:///asgp8buc...|\n",
      "|{file:///asgp8buc...|\n",
      "|{file:///asgp8buc...|\n",
      "|{file:///asgp8buc...|\n",
      "|{file:///asgp8buc...|\n",
      "|{file:///asgp8buc...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"image\").load(s3_url)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fab3f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('label', split(col('image')[\"origin\"], '/').getItem(8))\n",
    "df = df.withColumn('img', split(col('image')[\"origin\"], '/').getItem(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4febada4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----------+--------------------+\n",
      "|               image| label|        img|                vecs|\n",
      "+--------------------+------+-----------+--------------------+\n",
      "|{file:///asgp8buc...| apple|  0_100.jpg|[248.0,254.0,255....|\n",
      "|{file:///asgp8buc...| apple|101_100.jpg|[251.0,253.0,254....|\n",
      "|{file:///asgp8buc...| apple|100_100.jpg|[255.0,253.0,252....|\n",
      "|{file:///asgp8buc...|banana|  0_100.jpg|[255.0,253.0,255....|\n",
      "|{file:///asgp8buc...|banana|104_100.jpg|[251.0,255.0,254....|\n",
      "|{file:///asgp8buc...|banana|107_100.jpg|[254.0,255.0,253....|\n",
      "+--------------------+------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ImageSchema.imageFields\n",
    "\n",
    "img2vec = pyspark.sql.functions.udf(lambda x: DenseVector(ImageSchema.toNDArray(x).flatten()), VectorUDT())\n",
    "\n",
    "df = df.withColumn('vecs', img2vec(\"image\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e9e67",
   "metadata": {},
   "source": [
    "# 3 Principal Components Analysis\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511f1d1f",
   "metadata": {},
   "source": [
    "Input of PCA must be scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "109a3edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "standardizer = StandardScaler(withMean=True, withStd=True, inputCol='vecs', outputCol='vecs_scaled')\n",
    "df_tmp = standardizer.fit(df)\n",
    "std_df = df_tmp.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "087d9f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- img: string (nullable = true)\n",
      " |-- vecs: vector (nullable = true)\n",
      " |-- vecs_scaled: vector (nullable = true)\n",
      "\n",
      "+--------------------+------+-----------+--------------------+--------------------+\n",
      "|               image| label|        img|                vecs|         vecs_scaled|\n",
      "+--------------------+------+-----------+--------------------+--------------------+\n",
      "|{file:///asgp8buc...| apple|  0_100.jpg|[248.0,254.0,255....|[-1.5449937377599...|\n",
      "|{file:///asgp8buc...| apple|101_100.jpg|[251.0,253.0,254....|[-0.4753826885415...|\n",
      "|{file:///asgp8buc...| apple|100_100.jpg|[255.0,253.0,252....|[0.95076537708305...|\n",
      "|{file:///asgp8buc...|banana|  0_100.jpg|[255.0,253.0,255....|[0.95076537708305...|\n",
      "|{file:///asgp8buc...|banana|104_100.jpg|[251.0,255.0,254....|[-0.4753826885415...|\n",
      "|{file:///asgp8buc...|banana|107_100.jpg|[254.0,255.0,253....|[0.59422836067690...|\n",
      "+--------------------+------+-----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "std_df.printSchema()\n",
    "std_df.show(8) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e910b",
   "metadata": {},
   "source": [
    "## PCA fit transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0ca07d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/17 16:07:17 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/01/17 16:07:17 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/01/17 16:07:20 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "22/01/17 16:07:20 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.5 ms, sys: 8.47 ms, total: 53.9 ms\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pca = PCA(k=7, inputCol=\"vecs_scaled\", outputCol=\"pca\")\n",
    "modelpca = pca.fit(std_df)\n",
    "transformed = modelpca.transform(std_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a37ebf",
   "metadata": {},
   "source": [
    "# Export PCs to S3 as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a367c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transformed_final = transformed.select('img', 'label', 'pca')\n",
    "transformed_pandas = transformed_final.toPandas()\n",
    "transformed_pandas.to_csv('PCA_output_7_PCs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
